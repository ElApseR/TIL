## Shannon의 정보이론
- bit란?
    - 1bit의 정보를 보내는 것은, 정보 수신자의 불확실성을 2로 나누는 것이다.
    - ex) 맑음, 흐림 2개의 가능한 날씨(발생확률은 동일)가 있는데, 기상청이 흐릴 것이라고 예보를 했다.
        - 기상청은 옳다고 가정한다
        - single bit of information을 보낸 것이다. 불확실성이 2로 나뉘어서, 두 가지의 가능성이 한 가지로 줄어들었다.
        - 여기서 bit란 저장 용량과는 다른 개념이다. ‘rainy’라는 단어를 예보했다면 이것은  40 bit라는 용량을 가지지만, 그 정보의 양은 1 bit인 것이다.
    - ex) 여덟 개의 가능한 날씨(발생확률은 동일)가 있다면?
        - 한 개의 날씨를 전달하면, 수신자가 갖고 있는 8개의 불확실성을  2^3 으로 나눠준 게 되니까 3bit의 정보를 담고 있는 것이다.
        - 쉽게 bit를 계산하려면, uncertainty의 갯수를 밑이 2인 log를 취해주면 된다
    - ex) 맑음 확률 75%, 비올 확률 25% 라면?
        - 비올 것이다라고 예보를 했다면, 4만큼의 uncertainty를 줄인 것이므로 2 bit이다.
        - 불확실성 감소(uncertainty reduction)은 사실, event의 probability의 역수와 동일하다.
            - 따라서 25%의 역수인 4만큼의 uncertainty가 감소했다고 보고 bit를 계산한 것이다.
        - 이때, log{1/(1/4)} = -log(1/4) 이다. 즉, 원래 확률에 그대로 log 취하고 부호 바꿔준 것이다.
        - 같은 방식으로 맑다고 예보했을 때의 information은 -log2(0.75)=0.41 bit이다.
    - 위의 예시에서, 기상청 예보에 대한 기대 information은?
        - 0.75x(-log2(0.75) + 0.25x(-log2(0.25)) = 0.75x0.41 + 0.25x2 = 0.81 bit
        - 우리는 기상청으로부터 0.81bit의 정보를 얻을 것이라고 기대한다.
        - 이것이 entropy이다.
- entropy란?
    - event가 얼마나 uncertain한지를 나타내는 척도
    - 우리가 어떠한 probability distribution으로부터 sample을 뽑아냈을 때, 그 sample로부터 얻을 수 있는 정보의 양.  
        - 따라서 이를 통해 우리의 probability distribution이 얼마나 예측하기 어려운지를 측정할 수 있다.
## Cross Entropy
- average message length로 이해하자
- ex) 각각의 확률이 다른 8개의 기상 예보를 보내는 상황
    - 각각의 기상에 대해서 메시지를 전달해야한다.
    - 만약 8개니까 세자리의 2진수로 전달한다면, 평균적으로 3bit를 전달하지만, 수신자는 그보다 적은 bit의 정보를 얻는다(2.41bit라 하자)
    - 이를 개선하기 위해서, 각 날씨가 발생하는 확률에 맞게, 확률이 낮은 날씨는 긴 자릿수의 2진수를 이용하고, 높은 날씨는 짧은 자릿수의 2진수를 이용하자
        - 더 짧은 메시지를 더 자주 보내니까 평균은 확 낮아질 것이다.
        - 위와 반대라면 오히려 평균 전달 bit 는 오히려 늘어날 것이다.
    - 따라서 이처럼 전달하는 메시지의 길이가, 결국 실제 확률에 비례하는 것이 최선이 된다.
        - 전달하는 메시지의 길이를 우리가 추정하는 실제 확률이라고 생각하면 어떨까?!
    - formula
        - sum(-p_i*log_2(q_i))
        - p는 true 확률, q는 predicted 확률
            - 그냥 entropy는 둘 다 p_i였다.
    - 확률 분포를 정확히 예측한다면(=메시지의 길이가 정확히 각 날씨의 확률에 비례하면) cross entropy는 entropy와 동일해진다.
        - cross entropy와 entroty의 차이를 relative entropy라고 한다.
        - 혹은 KL divergence라고 한다
- 실제로 사용할때는 자연로그를 사용한다. 이는 밑이 2인 로그에 log(2)만큼 상수배한 것과 동일하므로 학습 과정에서
