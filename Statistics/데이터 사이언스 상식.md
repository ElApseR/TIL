# 데이터 사이언스 상식

*이 글은 <a href="https://zzsza.github.io/data/2018/02/17/datascience-interivew-questions/#%EB%94%A5%EB%9F%AC%EB%8B%9D">데이터 사이언스 인터뷰 질문 모음집</a>에 짧게 답을 달아보는 글입니다.*

*좋은 참고자료 : <a href="https://developers.google.com/machine-learning/glossary/?hl=ko">구글 머신러닝 용어집</a>*

## 머신러닝

- **Cross Validation은 무엇이고 어떻게 해야하나요?**
    - 일반적으로 머신러닝에서 풀고자 하는 문제는 예측 문제이며, 이에 대해 모형의 성능을 정확히 파악하기 위해서는 학습에 사용하지 않은 데이터에 대한 모델의 예측력을 보아야한다. 이는 학습 데이터에서만 성능이 좋은 overfitting 문제가 있을 수 있기 때문이다. 이를 위해 학습에 사용할 데이터와 그렇지 않을 데이터를 나누어야할 필요성이 생기며, 학습에 사용하지 않은 데이터로 모델을 평가하는 것을 cross validation이라 한다. 하지만 데이터의 갯수가 적어서 딱 한 번 데이터의 일부를 제외하여 모델을 평가하기에는 신뢰성이 낮은 경우, 데이터를 k개의 조각으로 나누어(일반적으로 다섯 개) cross validation을 수행한 뒤, k번의 성능의 평균으로 모델의 성능을 평가하는 경우도 있다. 이를 k-fold cross validation이라 한다.
- **회귀 / 분류시 알맞은 metric은 무엇일까요? 알고 있는 metric에 대해 설명해주세요(ex. RMSE, MAE, recall, precision …)**
    - 우선 metric이란 우리의 모델의 성능을 평가하는 척도이다. 따라서 metric을 기준으로 모델을 학습할 수 있다면 최고겠으나, 그렇지 못한 다양한 경우가 발생하며 이 경우 loss function을 활용하곤 한다. 선형회귀는 metric을 기준으로 모델을 학습할 수 있는 좋은 경우이다. 일반적으로 선형회귀에서 회귀계수를 구할 때는 종속변수(y)의 MSE를 최소화 시키는 값을 찾는다. 또한 선형회귀식의 성능을 평가할 때 자주 쓰는 metric은 R-square인데 이는 MSE가 작아질 수록 그 값이 커진다(RMSE는 square root of mean squared error이며, 이를 metric으로 사용해도 MSE에서의 대소관계는 유지되므로 결과적으로 동일하다). 분류의 경우 모형의 목적에 따라 accuracy를 이용할 수도 있고, recall과 precision 또는 이를 동시에 볼 수 있는 f1 score를 이용하기도 한다. 가장 일반적으로는 f1 score를 이용하며, 이는 precision과 recall의 조화평균이다. precision은 참긍정/(참긍정+거짓긍정) 이며, recall은 참긍정/(참긍정+거짓부정)이다. 즉, precision은 모델이 true라고 예측한 값 중 실제 true의 비율이며, recall은 실제 true 중 모델이 true라고 예측한 비율이다.
- **정규화를 왜 해야할까요? 정규화의 방법은 무엇이 있나요?**
    - 정규화란 단어가 사실 한국말로 중의적이라 애매한데, 여기서는 regularization을 의미하는 것이라 생각하겠다. 일반적으로 머신러닝 모델의 성능은 bias variance trade off가 존재한다. bias가 큰 모델은 데이터의 특성을 전부 뽑아내지 못하고 underfit된 모델이다. 이와 달리 variance가 큰 모델은 training 데이터의 특성에 너무 과하게 맞춰진 overfit된 모델이다. bias가 큰 모델은 그 어떤 test 데이터에 대해서도 비슷한 성능을 보여준다는 장점이 있으나, 그 성능이 좋지 않다는 단점이 있다. variance가 큰 모델은 test 데이터가 바뀔 때마다 성능이 매우 크게 변동한다. 어떨 때는 엄청 좋다가 어떨 때는 엄청 구리다. 두 경우 모두 좋지 않다는 것은 분명하며, 우리는 모든 test set에 대해서 높은 성능을 일정하게 내주는 모델을 학습하길 원한다. Regularization은 overfit된(variance가 큰) 모델의 variance를 줄여준다. 그 방식은 L1정규화(parameter의 절대값의 합에 제약) L2정규화(parameter의 제곱합에 제약)이 있으며, L0정규화(parameter 갯수에 제약) 등의 방법도 존재한다. 즉, 일부러 train데이터에 좀더 underfit되게 만드는 것이다. 추가적으로 L1 정규화항이 있는 선형회귀식은 lasso 회귀라고 하며, feature selection에도 활용되곤 한다.
- **Local Minima와 Global Minima에 대해 설명해주세요.**
    - <img src="https://t1.daumcdn.net/cfile/tistory/9915A83E5AB8621703">
        - 출처 : <a href="https://gomguard.tistory.com/187">머신러닝[딥러닝] 뉴럴 네트워크 Part. 8 - 옵티마이저 (Optimizer)</a>
    - 모델을 학습하는 데에 위에 metric에서 언급하였듯이 loss function을 활용하는 경우가 다수 존재한다. 이때, loss function을 최소화하는 parameter를 optimization을 활용하여 구한다. 가장 많이 사용되는 optimization 방법인 gradient descent(경사하강법)은 각 parameter space에서 loss function이 줄어드는 방향으로 parameter를 일정량 이동시키는 방법이다. 이때, 만약 loss function의 생김새가 조그만 구덩이(local mimima)와 전체적으로 큰 구덩이(global minima)가 섞여있는 형태라면, 우리의 시작점이 어디인지에 따라 & 이동하는 크기가 얼마인지에 따라 조그만 구덩이에서 optimization이 끝나버리는 경우가 발생한다. 즉, 최고로 loss가 낮은 모델(global optima)이 되지 않고 애매하게 loss가 낮은 모델(local optima)이 만들어지는 것이다. 이를 해결하기 위해선 이동하는 크기(learning rate)와 optimizer를 바꾸어주어야 한다. 최근 딥러닝에서 가장 많이 사용하는 adam optimizer는 관성(방향)과 보폭(크기)을 함께 고려하는 방식을 사용한다.
