# Cosine Similarity

- 참고
    - <a href="https://en.wikipedia.org/wiki/Cosine_similarity">위키피디아</a>
    - <a href="http://euriion.com/?p=548">Total Data Science</a>
    - <a href="https://wikidocs.net/24603">딥 러닝을 이용한 자연어 처리 입문</a>

---

## intro
- 텍스트 마이닝이나 추천 시스템을 공부하다보면 코사인 유사도가 자주 등장하는데, 이게 대체 무엇이고 왜 유사도를 나타내는 용도로 사용될 수 있는지 알아보았다. 추가적으로 유사도로 사용할 수 있는 다른 개념들도 알아볼 생각이다.

## 정의
- 코사인 유사도는 두 벡터가 이루는 각의 크기를 코사인으로 표현한 것이다.
    - cosine은 두 벡터의 방향이 완전히 동일하면(내각이 0도) 1의 값을 가진다
    - cosine은 두 벡터의 방향이 완전히 반대이면(내각이 180도) -1의 값을 가진다.
- 코사인은 벡터의 크기에는 영향을 받지 않는다. 오로지 두 벡터가 이루는 각도만으로 유사도를 파악한다.
- 실제 코사인 유사도는 백터의 성분이 모두 양의 값을 가져야만 한다. 즉, 모든 점이 1사분면에 존재해야하며, 그 값은 [0,1]의 범위를 가진다.
    - 1사분면에서 서로 다른 두 벡터가 가질 수 있는 최대 내각은 90도이기 때문이다.
- 수식은 아래와 같다.
    - <img scr="https://neo4j.com/docs/graph-algorithms/current/images/cosine-similarity.png">
        - from "neo4j"

## 활용
- 코사인 유사도가 가장 많이 활용되는 곳은 텍스트 마이닝이다. 특히 문서간의 유사도를 구할 때 많이 활용된다. 현재는 좀더 복잡한 걸 활용하지만, 과거에 많이 활용되었고 아직도 기초 개념으로 많이 활용된다.
- 방법은 문서 내에 존재하는 단어를 count 하여 그 결과를 해당 문서를 대표하는 벡터로 활용하는 것이다.

- 예)

||바나나|사과	|저는	|좋아요|
|---|---|---|---|---|
|문서1|	0|	1|	1|	1|
|문서2|	1|	0|	1|	1|
|문서3|	2|	0|	2|	2|

- 위 예에서 1&2, 1&3의 코사인 유사도는 0.63이다. 또한 2&3의 코사인 유사도는 1이다. 즉, 코사인 유사도는 크기와 무관하게 오로지 방향만으로 판단한다는 사실을 확인할 수 있다.

## 기타

- euclidean distance와 종종 비교가 되곤 한다. 유클리드 거리와 가장 큰 차이는 벡터의 크기가 영향을 미치지 않는다는 것이다. 예를 들어 위의 예에서 문서1과 문서3의 값에 매우 큰 수를 곱하고 각 문서간의 유클리드 거리를 구하면 1과 3의 거리가 2와 3의 거리보다 가깝게 측정될 것이다. 본래의 의도가 그것이 아니라면 의도와 다른 결과를 가져올 수 있는 것이다.
- 코사인 유사도가 얼마 이상이면 좋다라는 특별한 기준은 없다. 코사인 유사도는 상대적인 개념으로 사용되며, 대표적으로 군집 알고리즘에서 활용할 때 역시 그러하다.
- 코사인 유사도는 검색엔진에서 query에 관련이 높은 검색 결과를 제공할 때에 많이 사용되었다(현재는 더 복잡하다). 이 방식의 장점은 query는 상대적으로 단순한 내용을 담고 있는데 검색 결과는 방대한 내용(단어 수도 많고, 한 단어당 등장 횟수도 많음)을 담고 있기 때문에 유클리드 거리로는 유사도의 측정이 불가능하단 한계를 극복한 것이다. 즉, 내가 찾는 내용과 결과의 내용이 비슷한 방향으로 향하고 있는지를 파악할 수 있다.
